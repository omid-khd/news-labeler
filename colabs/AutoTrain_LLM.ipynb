{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "JvMRbVLEJlZT",
        "outputId": "e5c5d51a-9ed1-40d4-9d81-aa1787a022e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoTrain version: 0.8.36\n"
          ]
        }
      ],
      "source": [
        "#@title ðŸ¤— AutoTrain LLM\n",
        "#@markdown In order to use this colab\n",
        "#@markdown - upload train.csv to a folder named `data/`\n",
        "#@markdown - train.csv must contain a `text` column\n",
        "#@markdown - choose a project name if you wish\n",
        "#@markdown - change model if you wish, you can use most of the text-generation models from Hugging Face Hub\n",
        "#@markdown - add huggingface information (token) if you wish to push trained model to huggingface hub\n",
        "#@markdown - update hyperparameters if you wish\n",
        "#@markdown - click `Runtime > Run all` or run each cell individually\n",
        "#@markdown - report issues / feature requests here: https://github.com/huggingface/autotrain-advanced/issues\n",
        "\n",
        "\n",
        "import os\n",
        "!pip install -U autotrain-advanced > install_logs.txt 2>&1\n",
        "!autotrain setup --colab > setup_logs.txt\n",
        "from autotrain import __version__\n",
        "print(f'AutoTrain version: {__version__}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A2-_lkBS1WKA"
      },
      "outputs": [],
      "source": [
        "conf = f\"\"\"\n",
        "# yaml\n",
        "task: text_classification\n",
        "base_model: HooshvareLab/bert-fa-base-uncased\n",
        "project_name: news-labeler-high-acc\n",
        "log: tensorboard\n",
        "backend: local\n",
        "\n",
        "data:\n",
        "  path: '/content/data'\n",
        "  train_split: train\n",
        "  valid_split: valid\n",
        "  column_mapping:\n",
        "    text_column: text\n",
        "    target_column: target\n",
        "\n",
        "params:\n",
        "  max_seq_length: 512\n",
        "  epochs: 20\n",
        "  batch_size: 32\n",
        "  lr: 2e-5\n",
        "  optimizer: adamw_torch\n",
        "  scheduler: cosine\n",
        "  warmup_ratio: 0.1\n",
        "  weight_decay: 0.01\n",
        "  gradient_accumulation: 1\n",
        "  mixed_precision: fp16\n",
        "  early_stopping_patience: 4\n",
        "  evaluation_strategy: epoch\n",
        "  save_total_limit: 3\n",
        "  load_best_model_at_end: true\n",
        "  label_smoothing: 0.1\n",
        "  class_weights: auto\n",
        "\"\"\"\n",
        "\n",
        "with open(\"config.yaml\", \"w\") as f:\n",
        "  f.write(conf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "id": "g3cd_ED_yXXt",
        "outputId": "1aee9203-fd32-4652-b942-181e0be06687",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.cli.autotrain\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mUsing AutoTrain configuration: config.yaml\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1mRunning task: text_multi_class_classification\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m166\u001b[0m - \u001b[1mUsing backend: local\u001b[0m\n",
            "\u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m286\u001b[0m - \u001b[33m\u001b[1mParameters supplied but not used: evaluation_strategy, label_smoothing, class_weights, load_best_model_at_end\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m224\u001b[0m - \u001b[1m{'data_path': '/content/data', 'model': 'HooshvareLab/bert-fa-base-uncased', 'lr': 2e-05, 'epochs': 20, 'max_seq_length': 512, 'batch_size': 32, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'cosine', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'train_split': 'train', 'valid_split': 'valid', 'text_column': 'text', 'target_column': 'target', 'logging_steps': -1, 'project_name': 'news-labeler-high-acc', 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'save_total_limit': 3, 'token': None, 'push_to_hub': False, 'eval_strategy': 'epoch', 'username': None, 'log': 'tensorboard', 'early_stopping_patience': 4, 'early_stopping_threshold': 0.01}\u001b[0m\n",
            "\rCasting the dataset:   0% 0/1000 [00:00<?, ? examples/s]\rCasting the dataset: 100% 1000/1000 [00:00<00:00, 236725.59 examples/s]\n",
            "\rCasting the dataset:   0% 0/1000 [00:00<?, ? examples/s]\rCasting the dataset: 100% 1000/1000 [00:00<00:00, 232926.31 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/1000 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 1000/1000 [00:00<00:00, 278746.86 examples/s]\rSaving the dataset (1/1 shards): 100% 1000/1000 [00:00<00:00, 265933.55 examples/s]\n",
            "\rSaving the dataset (0/1 shards):   0% 0/1000 [00:00<?, ? examples/s]\rSaving the dataset (1/1 shards): 100% 1000/1000 [00:00<00:00, 284552.51 examples/s]\rSaving the dataset (1/1 shards): 100% 1000/1000 [00:00<00:00, 270897.37 examples/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.backends.local\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mStarting local training...\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m514\u001b[0m - \u001b[1m['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.text_classification', '--training_config', 'news-labeler-high-acc/training_params.json']\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:06\u001b[0m | \u001b[36mautotrain.commands\u001b[0m:\u001b[36mlaunch_command\u001b[0m:\u001b[36m515\u001b[0m - \u001b[1m{'data_path': 'news-labeler-high-acc/autotrain-data', 'model': 'HooshvareLab/bert-fa-base-uncased', 'lr': 2e-05, 'epochs': 20, 'max_seq_length': 512, 'batch_size': 32, 'warmup_ratio': 0.1, 'gradient_accumulation': 1, 'optimizer': 'adamw_torch', 'scheduler': 'cosine', 'weight_decay': 0.01, 'max_grad_norm': 1.0, 'seed': 42, 'train_split': 'train', 'valid_split': 'validation', 'text_column': 'autotrain_text', 'target_column': 'autotrain_label', 'logging_steps': -1, 'project_name': 'news-labeler-high-acc', 'auto_find_batch_size': False, 'mixed_precision': 'fp16', 'save_total_limit': 3, 'token': None, 'push_to_hub': False, 'eval_strategy': 'epoch', 'username': None, 'log': 'tensorboard', 'early_stopping_patience': 4, 'early_stopping_threshold': 0.01}\u001b[0m\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1767725719.584337    3864 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1767725719.591164    3864 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1767725719.608069    3864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767725719.608109    3864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767725719.608114    3864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1767725719.608120    3864 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:24\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:24\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
            "config.json: 100% 440/440 [00:00<00:00, 3.31MB/s]\n",
            "pytorch_model.bin: 100% 654M/654M [00:15<00:00, 42.7MB/s]\n",
            "model.safetensors:   0% 0.00/654M [00:00<?, ?B/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "model.safetensors:   3% 21.0M/654M [00:00<00:19, 32.3MB/s]\n",
            "model.safetensors:   5% 31.5M/654M [00:00<00:14, 43.2MB/s]\n",
            "vocab.txt: 1.20MB [00:00, 7.66MB/s]\n",
            "model.safetensors:  10% 62.9M/654M [00:01<00:09, 63.8MB/s]\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:44\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m147\u001b[0m - \u001b[1mLogging steps: 6\u001b[0m\n",
            "model.safetensors:  19% 126M/654M [00:02<00:10, 49.8MB/s]\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:45\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_train_begin\u001b[0m:\u001b[36m386\u001b[0m - \u001b[1mStarting to train...\u001b[0m\n",
            "\n",
            "model.safetensors:  35% 231M/654M [00:04<00:07, 59.3MB/s]\n",
            "model.safetensors:  40% 262M/654M [00:04<00:05, 72.0MB/s]\n",
            "model.safetensors:  46% 304M/654M [00:05<00:06, 52.9MB/s]\n",
            "model.safetensors:  53% 346M/654M [00:06<00:05, 60.2MB/s]\n",
            "model.safetensors:  61% 398M/654M [00:07<00:03, 73.2MB/s]\n",
            "model.safetensors:  66% 430M/654M [00:07<00:04, 52.5MB/s]\n",
            "  1% 6/640 [00:05<08:00,  1.32it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:50\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 2.3823, 'grad_norm': 13.330440521240234, 'learning_rate': 1.8750000000000003e-06, 'epoch': 0.1875}\u001b[0m\n",
            "                                                         \n",
            "\u001b[A{'loss': 2.3823, 'grad_norm': 13.330440521240234, 'learning_rate': 1.8750000000000003e-06, 'epoch': 0.19}\n",
            "model.safetensors:  66% 430M/654M [00:07<00:04, 52.5MB/s]\n",
            "model.safetensors:  71% 461M/654M [00:08<00:03, 55.4MB/s]\n",
            "model.safetensors:  77% 503M/654M [00:09<00:02, 55.6MB/s]\n",
            "model.safetensors:  82% 535M/654M [00:09<00:01, 61.8MB/s]\n",
            "model.safetensors:  88% 577M/654M [00:10<00:01, 50.1MB/s]\n",
            "model.safetensors:  95% 619M/654M [00:11<00:00, 53.4MB/s]\n",
            "model.safetensors: 100% 654M/654M [00:11<00:00, 54.6MB/s]\n",
            "\n",
            "  2% 12/640 [00:09<07:24,  1.41it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 2.2609, 'grad_norm': 10.245689392089844, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 2.2609, 'grad_norm': 10.245689392089844, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.38}\n",
            "\n",
            "  2% 12/640 [00:09<07:24,  1.41it/s]\u001b[A\n",
            "  2% 13/640 [00:10<07:20,  1.42it/s]\u001b[A\n",
            "  2% 14/640 [00:10<07:17,  1.43it/s]\u001b[A\n",
            "  2% 15/640 [00:11<07:14,  1.44it/s]\u001b[A\n",
            "  2% 16/640 [00:12<07:13,  1.44it/s]\u001b[A\n",
            "  3% 17/640 [00:13<07:11,  1.44it/s]\u001b[A\n",
            "  3% 18/640 [00:13<07:11,  1.44it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:55:59\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 2.0046, 'grad_norm': 10.089987754821777, 'learning_rate': 5.625e-06, 'epoch': 0.5625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 2.0046, 'grad_norm': 10.089987754821777, 'learning_rate': 5.625e-06, 'epoch': 0.56}\n",
            "\n",
            "  3% 18/640 [00:13<07:11,  1.44it/s]\u001b[A\n",
            "  3% 19/640 [00:14<07:11,  1.44it/s]\u001b[A\n",
            "  3% 20/640 [00:15<07:09,  1.44it/s]\u001b[A\n",
            "  3% 21/640 [00:15<07:10,  1.44it/s]\u001b[A\n",
            "  3% 22/640 [00:16<07:09,  1.44it/s]\u001b[A\n",
            "  4% 23/640 [00:17<07:11,  1.43it/s]\u001b[A\n",
            "  4% 24/640 [00:17<07:09,  1.43it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:03\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.807, 'grad_norm': 7.711770057678223, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.75}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.807, 'grad_norm': 7.711770057678223, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.75}\n",
            "\n",
            "  4% 24/640 [00:17<07:09,  1.43it/s]\u001b[A\n",
            "  4% 25/640 [00:18<07:10,  1.43it/s]\u001b[A\n",
            "  4% 26/640 [00:19<07:12,  1.42it/s]\u001b[A\n",
            "  4% 27/640 [00:20<07:14,  1.41it/s]\u001b[A\n",
            "  4% 28/640 [00:20<07:15,  1.40it/s]\u001b[A\n",
            "  5% 29/640 [00:21<07:14,  1.41it/s]\u001b[A\n",
            "  5% 30/640 [00:22<07:12,  1.41it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.611, 'grad_norm': 4.225487232208252, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.9375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.611, 'grad_norm': 4.225487232208252, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.94}\n",
            "\n",
            "  5% 30/640 [00:22<07:12,  1.41it/s]\u001b[A\n",
            "  5% 31/640 [00:22<07:08,  1.42it/s]\u001b[A\n",
            "100% 16/16 [00:06<00:00,  2.68it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 1.529605507850647, 'eval_f1_macro': 0.06953685583822569, 'eval_f1_micro': 0.533, 'eval_f1_weighted': 0.370631441617743, 'eval_precision_macro': 0.0533, 'eval_precision_micro': 0.533, 'eval_precision_weighted': 0.284089, 'eval_recall_macro': 0.1, 'eval_recall_micro': 0.533, 'eval_recall_weighted': 0.533, 'eval_accuracy': 0.533, 'eval_runtime': 6.8328, 'eval_samples_per_second': 146.353, 'eval_steps_per_second': 2.342, 'epoch': 1.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.529605507850647, 'eval_f1_macro': 0.06953685583822569, 'eval_f1_micro': 0.533, 'eval_f1_weighted': 0.370631441617743, 'eval_precision_macro': 0.0533, 'eval_precision_micro': 0.533, 'eval_precision_weighted': 0.284089, 'eval_recall_macro': 0.1, 'eval_recall_micro': 0.533, 'eval_recall_weighted': 0.533, 'eval_accuracy': 0.533, 'eval_runtime': 6.8328, 'eval_samples_per_second': 146.353, 'eval_steps_per_second': 2.342, 'epoch': 1.0}\n",
            "100% 16/16 [00:06<00:00,  2.68it/s]\n",
            "\n",
            "  5% 33/640 [00:43<1:05:54,  6.52s/it]\u001b[A\n",
            "  5% 34/640 [00:44<48:13,  4.78s/it]  \u001b[A\n",
            "  5% 35/640 [00:44<35:53,  3.56s/it]\u001b[A\n",
            "  6% 36/640 [00:45<27:13,  2.70s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:31\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.3382, 'grad_norm': 4.294233798980713, 'learning_rate': 1.125e-05, 'epoch': 1.125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.3382, 'grad_norm': 4.294233798980713, 'learning_rate': 1.125e-05, 'epoch': 1.12}\n",
            "\n",
            "  6% 36/640 [00:45<27:13,  2.70s/it]\u001b[A\n",
            "  6% 37/640 [00:46<21:10,  2.11s/it]\u001b[A\n",
            "  6% 38/640 [00:47<16:55,  1.69s/it]\u001b[A\n",
            "  6% 39/640 [00:47<13:58,  1.40s/it]\u001b[A\n",
            "  6% 40/640 [00:48<11:53,  1.19s/it]\u001b[A\n",
            "  6% 41/640 [00:49<10:27,  1.05s/it]\u001b[A\n",
            "  7% 42/640 [00:49<09:27,  1.05it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:35\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.4052, 'grad_norm': 4.470923900604248, 'learning_rate': 1.3125e-05, 'epoch': 1.3125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.4052, 'grad_norm': 4.470923900604248, 'learning_rate': 1.3125e-05, 'epoch': 1.31}\n",
            "\n",
            "  7% 42/640 [00:49<09:27,  1.05it/s]\u001b[A\n",
            "  7% 43/640 [00:50<08:43,  1.14it/s]\u001b[A\n",
            "  7% 44/640 [00:51<08:13,  1.21it/s]\u001b[A\n",
            "  7% 45/640 [00:52<07:52,  1.26it/s]\u001b[A\n",
            "  7% 46/640 [00:52<07:38,  1.30it/s]\u001b[A\n",
            "  7% 47/640 [00:53<07:26,  1.33it/s]\u001b[A\n",
            "  8% 48/640 [00:54<07:18,  1.35it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:39\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.2348, 'grad_norm': 3.9098663330078125, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.5}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.2348, 'grad_norm': 3.9098663330078125, 'learning_rate': 1.5000000000000002e-05, 'epoch': 1.5}\n",
            "\n",
            "  8% 48/640 [00:54<07:18,  1.35it/s]\u001b[A\n",
            "  8% 49/640 [00:54<07:14,  1.36it/s]\u001b[A\n",
            "  8% 50/640 [00:55<07:14,  1.36it/s]\u001b[A\n",
            "  8% 51/640 [00:56<07:12,  1.36it/s]\u001b[A\n",
            "  8% 52/640 [00:57<07:13,  1.36it/s]\u001b[A\n",
            "  8% 53/640 [00:57<07:09,  1.37it/s]\u001b[A\n",
            "  8% 54/640 [00:58<07:04,  1.38it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:44\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 1.2124, 'grad_norm': 5.407269477844238, 'learning_rate': 1.6875e-05, 'epoch': 1.6875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 1.2124, 'grad_norm': 5.407269477844238, 'learning_rate': 1.6875e-05, 'epoch': 1.69}\n",
            "\n",
            "  8% 54/640 [00:58<07:04,  1.38it/s]\u001b[A\n",
            "  9% 55/640 [00:59<07:03,  1.38it/s]\u001b[A\n",
            "  9% 56/640 [01:00<07:02,  1.38it/s]\u001b[A\n",
            "  9% 57/640 [01:00<07:01,  1.38it/s]\u001b[A\n",
            "  9% 58/640 [01:01<07:01,  1.38it/s]\u001b[A\n",
            "  9% 59/640 [01:02<07:02,  1.38it/s]\u001b[A\n",
            "  9% 60/640 [01:02<07:01,  1.38it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.8922, 'grad_norm': 6.418125152587891, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.8922, 'grad_norm': 6.418125152587891, 'learning_rate': 1.8750000000000002e-05, 'epoch': 1.88}\n",
            "\n",
            "  9% 60/640 [01:02<07:01,  1.38it/s]\u001b[A\n",
            " 10% 61/640 [01:03<07:02,  1.37it/s]\u001b[A\n",
            " 10% 62/640 [01:04<07:00,  1.37it/s]\u001b[A\n",
            " 10% 63/640 [01:05<06:58,  1.38it/s]\u001b[A\n",
            "100% 16/16 [00:06<00:00,  2.52it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:56:58\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.8687774538993835, 'eval_f1_macro': 0.30657708552399665, 'eval_f1_micro': 0.745, 'eval_f1_weighted': 0.6687946585874753, 'eval_precision_macro': 0.29788907549333393, 'eval_precision_micro': 0.745, 'eval_precision_weighted': 0.6197215144814039, 'eval_recall_macro': 0.34225325273784835, 'eval_recall_micro': 0.745, 'eval_recall_weighted': 0.745, 'eval_accuracy': 0.745, 'eval_runtime': 7.292, 'eval_samples_per_second': 137.136, 'eval_steps_per_second': 2.194, 'epoch': 2.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8687774538993835, 'eval_f1_macro': 0.30657708552399665, 'eval_f1_micro': 0.745, 'eval_f1_weighted': 0.6687946585874753, 'eval_precision_macro': 0.29788907549333393, 'eval_precision_micro': 0.745, 'eval_precision_weighted': 0.6197215144814039, 'eval_recall_macro': 0.34225325273784835, 'eval_recall_micro': 0.745, 'eval_recall_weighted': 0.745, 'eval_accuracy': 0.745, 'eval_runtime': 7.292, 'eval_samples_per_second': 137.136, 'eval_steps_per_second': 2.194, 'epoch': 2.0}\n",
            "100% 16/16 [00:06<00:00,  2.52it/s]\n",
            "\n",
            " 10% 65/640 [01:24<59:08,  6.17s/it]\u001b[A\n",
            " 10% 66/640 [01:25<43:25,  4.54s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:10\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.8387, 'grad_norm': 4.963716506958008, 'learning_rate': 1.99994050500015e-05, 'epoch': 2.0625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.8387, 'grad_norm': 4.963716506958008, 'learning_rate': 1.99994050500015e-05, 'epoch': 2.06}\n",
            "\n",
            " 10% 66/640 [01:25<43:25,  4.54s/it]\u001b[A\n",
            " 10% 67/640 [01:26<32:27,  3.40s/it]\u001b[A\n",
            " 11% 68/640 [01:26<24:47,  2.60s/it]\u001b[A\n",
            " 11% 69/640 [01:27<19:26,  2.04s/it]\u001b[A\n",
            " 11% 70/640 [01:28<15:40,  1.65s/it]\u001b[A\n",
            " 11% 71/640 [01:28<13:05,  1.38s/it]\u001b[A\n",
            " 11% 72/640 [01:29<11:12,  1.18s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:15\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.7151, 'grad_norm': 5.0945024490356445, 'learning_rate': 1.999048221581858e-05, 'epoch': 2.25}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.7151, 'grad_norm': 5.0945024490356445, 'learning_rate': 1.999048221581858e-05, 'epoch': 2.25}\n",
            "\n",
            " 11% 72/640 [01:29<11:12,  1.18s/it]\u001b[A\n",
            " 11% 73/640 [01:30<09:53,  1.05s/it]\u001b[A\n",
            " 12% 74/640 [01:31<08:59,  1.05it/s]\u001b[A\n",
            " 12% 75/640 [01:31<08:24,  1.12it/s]\u001b[A\n",
            " 12% 76/640 [01:32<07:58,  1.18it/s]\u001b[A\n",
            " 12% 77/640 [01:33<07:41,  1.22it/s]\u001b[A\n",
            " 12% 78/640 [01:34<07:31,  1.25it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:19\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.7337, 'grad_norm': 5.326333522796631, 'learning_rate': 1.9970861323044667e-05, 'epoch': 2.4375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.7337, 'grad_norm': 5.326333522796631, 'learning_rate': 1.9970861323044667e-05, 'epoch': 2.44}\n",
            "\n",
            " 12% 78/640 [01:34<07:31,  1.25it/s]\u001b[A\n",
            " 12% 79/640 [01:34<07:22,  1.27it/s]\u001b[A\n",
            " 12% 80/640 [01:35<07:14,  1.29it/s]\u001b[A\n",
            " 13% 81/640 [01:36<07:09,  1.30it/s]\u001b[A\n",
            " 13% 82/640 [01:37<07:05,  1.31it/s]\u001b[A\n",
            " 13% 83/640 [01:37<07:03,  1.32it/s]\u001b[A\n",
            " 13% 84/640 [01:38<07:00,  1.32it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.6046, 'grad_norm': 8.515714645385742, 'learning_rate': 1.9940563382223196e-05, 'epoch': 2.625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.6046, 'grad_norm': 8.515714645385742, 'learning_rate': 1.9940563382223196e-05, 'epoch': 2.62}\n",
            "\n",
            " 13% 84/640 [01:38<07:00,  1.32it/s]\u001b[A\n",
            " 13% 85/640 [01:39<06:58,  1.33it/s]\u001b[A\n",
            " 13% 86/640 [01:40<06:58,  1.32it/s]\u001b[A\n",
            " 14% 87/640 [01:40<06:55,  1.33it/s]\u001b[A\n",
            " 14% 88/640 [01:41<06:54,  1.33it/s]\u001b[A\n",
            " 14% 89/640 [01:42<06:53,  1.33it/s]\u001b[A\n",
            " 14% 90/640 [01:43<06:52,  1.33it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.5791, 'grad_norm': 6.725943565368652, 'learning_rate': 1.989962083714808e-05, 'epoch': 2.8125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.5791, 'grad_norm': 6.725943565368652, 'learning_rate': 1.989962083714808e-05, 'epoch': 2.81}\n",
            "\n",
            " 14% 90/640 [01:43<06:52,  1.33it/s]\u001b[A\n",
            " 14% 91/640 [01:43<06:52,  1.33it/s]\u001b[A\n",
            " 14% 92/640 [01:44<06:54,  1.32it/s]\u001b[A\n",
            " 15% 93/640 [01:45<06:56,  1.31it/s]\u001b[A\n",
            " 15% 94/640 [01:46<06:57,  1.31it/s]\u001b[A\n",
            " 15% 95/640 [01:47<06:54,  1.32it/s]\u001b[A\n",
            " 15% 96/640 [01:47<05:28,  1.65it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.6506, 'grad_norm': 6.617499828338623, 'learning_rate': 1.9848077530122083e-05, 'epoch': 3.0}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.6506, 'grad_norm': 6.617499828338623, 'learning_rate': 1.9848077530122083e-05, 'epoch': 3.0}\n",
            "\n",
            "100% 16/16 [00:07<00:00,  2.35it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:57:40\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.5995208621025085, 'eval_f1_macro': 0.45553396850398986, 'eval_f1_micro': 0.816, 'eval_f1_weighted': 0.7955077912577564, 'eval_precision_macro': 0.5649338968495263, 'eval_precision_micro': 0.816, 'eval_precision_weighted': 0.8276781374994817, 'eval_recall_macro': 0.46510354778411084, 'eval_recall_micro': 0.816, 'eval_recall_weighted': 0.816, 'eval_accuracy': 0.816, 'eval_runtime': 7.7708, 'eval_samples_per_second': 128.687, 'eval_steps_per_second': 2.059, 'epoch': 3.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.5995208621025085, 'eval_f1_macro': 0.45553396850398986, 'eval_f1_micro': 0.816, 'eval_f1_weighted': 0.7955077912577564, 'eval_precision_macro': 0.5649338968495263, 'eval_precision_micro': 0.816, 'eval_precision_weighted': 0.8276781374994817, 'eval_recall_macro': 0.46510354778411084, 'eval_recall_micro': 0.816, 'eval_recall_weighted': 0.816, 'eval_accuracy': 0.816, 'eval_runtime': 7.7708, 'eval_samples_per_second': 128.687, 'eval_steps_per_second': 2.059, 'epoch': 3.0}\n",
            "100% 16/16 [00:07<00:00,  2.35it/s]\n",
            "\n",
            " 15% 97/640 [02:13<1:14:13,  8.20s/it]\u001b[A\n",
            " 15% 98/640 [02:13<53:53,  5.97s/it]  \u001b[A\n",
            " 15% 99/640 [02:14<39:41,  4.40s/it]\u001b[A\n",
            " 16% 100/640 [02:15<29:48,  3.31s/it]\u001b[A\n",
            " 16% 101/640 [02:16<22:53,  2.55s/it]\u001b[A\n",
            " 16% 102/640 [02:16<18:03,  2.01s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:02\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.4418, 'grad_norm': 6.963108539581299, 'learning_rate': 1.9785988655009386e-05, 'epoch': 3.1875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.4418, 'grad_norm': 6.963108539581299, 'learning_rate': 1.9785988655009386e-05, 'epoch': 3.19}\n",
            "\n",
            " 16% 102/640 [02:17<18:03,  2.01s/it]\u001b[A\n",
            " 16% 103/640 [02:17<14:38,  1.64s/it]\u001b[A\n",
            " 16% 104/640 [02:18<12:18,  1.38s/it]\u001b[A\n",
            " 16% 105/640 [02:19<10:39,  1.20s/it]\u001b[A\n",
            " 17% 106/640 [02:20<09:31,  1.07s/it]\u001b[A\n",
            " 17% 107/640 [02:20<08:45,  1.01it/s]\u001b[A\n",
            " 17% 108/640 [02:21<08:13,  1.08it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:07\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.3877, 'grad_norm': 2.6869661808013916, 'learning_rate': 1.9713420698132614e-05, 'epoch': 3.375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.3877, 'grad_norm': 2.6869661808013916, 'learning_rate': 1.9713420698132614e-05, 'epoch': 3.38}\n",
            "\n",
            " 17% 108/640 [02:21<08:13,  1.08it/s]\u001b[A\n",
            " 17% 109/640 [02:22<07:47,  1.14it/s]\u001b[A\n",
            " 17% 110/640 [02:23<07:31,  1.17it/s]\u001b[A\n",
            " 17% 111/640 [02:23<07:18,  1.21it/s]\u001b[A\n",
            " 18% 112/640 [02:24<07:11,  1.22it/s]\u001b[A\n",
            " 18% 113/640 [02:25<07:05,  1.24it/s]\u001b[A\n",
            " 18% 114/640 [02:26<07:02,  1.25it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:11\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.4801, 'grad_norm': 3.974461555480957, 'learning_rate': 1.963045136707763e-05, 'epoch': 3.5625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.4801, 'grad_norm': 3.974461555480957, 'learning_rate': 1.963045136707763e-05, 'epoch': 3.56}\n",
            "\n",
            " 18% 114/640 [02:26<07:02,  1.25it/s]\u001b[A\n",
            " 18% 115/640 [02:27<06:59,  1.25it/s]\u001b[A\n",
            " 18% 116/640 [02:27<06:57,  1.26it/s]\u001b[A\n",
            " 18% 117/640 [02:28<06:55,  1.26it/s]\u001b[A\n",
            " 18% 118/640 [02:29<06:51,  1.27it/s]\u001b[A\n",
            " 19% 119/640 [02:30<06:50,  1.27it/s]\u001b[A\n",
            " 19% 120/640 [02:31<06:52,  1.26it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:16\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.3715, 'grad_norm': 5.474847793579102, 'learning_rate': 1.953716950748227e-05, 'epoch': 3.75}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.3715, 'grad_norm': 5.474847793579102, 'learning_rate': 1.953716950748227e-05, 'epoch': 3.75}\n",
            "\n",
            " 19% 120/640 [02:31<06:52,  1.26it/s]\u001b[A\n",
            " 19% 121/640 [02:31<06:51,  1.26it/s]\u001b[A\n",
            " 19% 122/640 [02:32<06:53,  1.25it/s]\u001b[A\n",
            " 19% 123/640 [02:33<06:55,  1.24it/s]\u001b[A\n",
            " 19% 124/640 [02:34<06:51,  1.25it/s]\u001b[A\n",
            " 20% 125/640 [02:35<06:49,  1.26it/s]\u001b[A\n",
            " 20% 126/640 [02:35<06:45,  1.27it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:21\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.3593, 'grad_norm': 4.481147289276123, 'learning_rate': 1.9433675007898255e-05, 'epoch': 3.9375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.3593, 'grad_norm': 4.481147289276123, 'learning_rate': 1.9433675007898255e-05, 'epoch': 3.94}\n",
            "\n",
            " 20% 126/640 [02:35<06:45,  1.27it/s]\u001b[A\n",
            " 20% 127/640 [02:36<06:43,  1.27it/s]\u001b[A\n",
            "100% 16/16 [00:07<00:00,  2.31it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.5250822305679321, 'eval_f1_macro': 0.5955198660866677, 'eval_f1_micro': 0.871, 'eval_f1_weighted': 0.8621172968066442, 'eval_precision_macro': 0.6505551290459826, 'eval_precision_micro': 0.871, 'eval_precision_weighted': 0.8588309403909599, 'eval_recall_macro': 0.5722152870471441, 'eval_recall_micro': 0.871, 'eval_recall_weighted': 0.871, 'eval_accuracy': 0.871, 'eval_runtime': 7.972, 'eval_samples_per_second': 125.439, 'eval_steps_per_second': 2.007, 'epoch': 4.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.5250822305679321, 'eval_f1_macro': 0.5955198660866677, 'eval_f1_micro': 0.871, 'eval_f1_weighted': 0.8621172968066442, 'eval_precision_macro': 0.6505551290459826, 'eval_precision_micro': 0.871, 'eval_precision_weighted': 0.8588309403909599, 'eval_recall_macro': 0.5722152870471441, 'eval_recall_micro': 0.871, 'eval_recall_weighted': 0.871, 'eval_accuracy': 0.871, 'eval_runtime': 7.972, 'eval_samples_per_second': 125.439, 'eval_steps_per_second': 2.007, 'epoch': 4.0}\n",
            "100% 16/16 [00:07<00:00,  2.31it/s]\n",
            "\n",
            " 20% 129/640 [03:00<1:03:58,  7.51s/it]\u001b[A\n",
            " 20% 130/640 [03:01<46:33,  5.48s/it]  \u001b[A\n",
            " 20% 131/640 [03:01<34:23,  4.05s/it]\u001b[A\n",
            " 21% 132/640 [03:02<25:56,  3.06s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.4053, 'grad_norm': 7.0733866691589355, 'learning_rate': 1.932007869282799e-05, 'epoch': 4.125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.4053, 'grad_norm': 7.0733866691589355, 'learning_rate': 1.932007869282799e-05, 'epoch': 4.12}\n",
            "\n",
            " 21% 132/640 [03:02<25:56,  3.06s/it]\u001b[A\n",
            " 21% 133/640 [03:03<20:00,  2.37s/it]\u001b[A\n",
            " 21% 134/640 [03:04<15:53,  1.88s/it]\u001b[A\n",
            " 21% 135/640 [03:04<12:59,  1.54s/it]\u001b[A\n",
            " 21% 136/640 [03:05<10:57,  1.31s/it]\u001b[A\n",
            " 21% 137/640 [03:06<09:32,  1.14s/it]\u001b[A\n",
            " 22% 138/640 [03:07<08:32,  1.02s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.3638, 'grad_norm': 6.4495368003845215, 'learning_rate': 1.9196502204050925e-05, 'epoch': 4.3125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.3638, 'grad_norm': 6.4495368003845215, 'learning_rate': 1.9196502204050925e-05, 'epoch': 4.31}\n",
            "\n",
            " 22% 138/640 [03:07<08:32,  1.02s/it]\u001b[A\n",
            " 22% 139/640 [03:07<07:51,  1.06it/s]\u001b[A\n",
            " 22% 140/640 [03:08<07:24,  1.12it/s]\u001b[A\n",
            " 22% 141/640 [03:09<07:05,  1.17it/s]\u001b[A\n",
            " 22% 142/640 [03:10<06:52,  1.21it/s]\u001b[A\n",
            " 22% 143/640 [03:10<06:40,  1.24it/s]\u001b[A\n",
            " 22% 144/640 [03:11<06:30,  1.27it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:58:57\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.2595, 'grad_norm': 3.51259708404541, 'learning_rate': 1.9063077870366504e-05, 'epoch': 4.5}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.2595, 'grad_norm': 3.51259708404541, 'learning_rate': 1.9063077870366504e-05, 'epoch': 4.5}\n",
            "\n",
            " 22% 144/640 [03:11<06:30,  1.27it/s]\u001b[A\n",
            " 23% 145/640 [03:12<06:25,  1.28it/s]\u001b[A\n",
            " 23% 146/640 [03:13<06:19,  1.30it/s]\u001b[A\n",
            " 23% 147/640 [03:13<06:15,  1.31it/s]\u001b[A\n",
            " 23% 148/640 [03:14<06:13,  1.32it/s]\u001b[A\n",
            " 23% 149/640 [03:15<06:15,  1.31it/s]\u001b[A\n",
            " 23% 150/640 [03:16<06:15,  1.30it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:01\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.2011, 'grad_norm': 6.494255065917969, 'learning_rate': 1.8919948565893144e-05, 'epoch': 4.6875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.2011, 'grad_norm': 6.494255065917969, 'learning_rate': 1.8919948565893144e-05, 'epoch': 4.69}\n",
            "\n",
            " 23% 150/640 [03:16<06:15,  1.30it/s]\u001b[A\n",
            " 24% 151/640 [03:17<06:14,  1.31it/s]\u001b[A\n",
            " 24% 152/640 [03:17<06:14,  1.30it/s]\u001b[A\n",
            " 24% 153/640 [03:18<06:14,  1.30it/s]\u001b[A\n",
            " 24% 154/640 [03:19<06:11,  1.31it/s]\u001b[A\n",
            " 24% 155/640 [03:20<06:11,  1.31it/s]\u001b[A\n",
            " 24% 156/640 [03:20<06:12,  1.30it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:06\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.2366, 'grad_norm': 12.79460334777832, 'learning_rate': 1.876726755707508e-05, 'epoch': 4.875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.2366, 'grad_norm': 12.79460334777832, 'learning_rate': 1.876726755707508e-05, 'epoch': 4.88}\n",
            "\n",
            " 24% 156/640 [03:20<06:12,  1.30it/s]\u001b[A\n",
            " 25% 157/640 [03:21<06:12,  1.30it/s]\u001b[A\n",
            " 25% 158/640 [03:22<06:18,  1.27it/s]\u001b[A\n",
            " 25% 159/640 [03:23<06:16,  1.28it/s]\u001b[A\n",
            "100% 16/16 [00:07<00:00,  2.25it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:17\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.522391140460968, 'eval_f1_macro': 0.6094926850865078, 'eval_f1_micro': 0.873, 'eval_f1_weighted': 0.8670084658645909, 'eval_precision_macro': 0.680614716490758, 'eval_precision_micro': 0.873, 'eval_precision_weighted': 0.8687680323062604, 'eval_recall_macro': 0.5853194504729733, 'eval_recall_micro': 0.873, 'eval_recall_weighted': 0.873, 'eval_accuracy': 0.873, 'eval_runtime': 8.0536, 'eval_samples_per_second': 124.168, 'eval_steps_per_second': 1.987, 'epoch': 5.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.522391140460968, 'eval_f1_macro': 0.6094926850865078, 'eval_f1_micro': 0.873, 'eval_f1_weighted': 0.8670084658645909, 'eval_precision_macro': 0.680614716490758, 'eval_precision_micro': 0.873, 'eval_precision_weighted': 0.8687680323062604, 'eval_recall_macro': 0.5853194504729733, 'eval_recall_micro': 0.873, 'eval_recall_weighted': 0.873, 'eval_accuracy': 0.873, 'eval_runtime': 8.0536, 'eval_samples_per_second': 124.168, 'eval_steps_per_second': 1.987, 'epoch': 5.0}\n",
            "100% 16/16 [00:07<00:00,  2.25it/s]\n",
            "\n",
            " 25% 161/640 [03:43<52:03,  6.52s/it]\u001b[A\n",
            " 25% 162/640 [03:44<38:09,  4.79s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:30\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.2482, 'grad_norm': 2.802659511566162, 'learning_rate': 1.860519833856079e-05, 'epoch': 5.0625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.2482, 'grad_norm': 2.802659511566162, 'learning_rate': 1.860519833856079e-05, 'epoch': 5.06}\n",
            "\n",
            " 25% 162/640 [03:44<38:09,  4.79s/it]\u001b[A\n",
            " 25% 163/640 [03:45<28:26,  3.58s/it]\u001b[A\n",
            " 26% 164/640 [03:46<21:41,  2.74s/it]\u001b[A\n",
            " 26% 165/640 [03:46<16:59,  2.15s/it]\u001b[A\n",
            " 26% 166/640 [03:47<13:37,  1.73s/it]\u001b[A\n",
            " 26% 167/640 [03:48<11:17,  1.43s/it]\u001b[A\n",
            " 26% 168/640 [03:49<09:39,  1.23s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:34\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.2087, 'grad_norm': 4.247127056121826, 'learning_rate': 1.843391445812886e-05, 'epoch': 5.25}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.2087, 'grad_norm': 4.247127056121826, 'learning_rate': 1.843391445812886e-05, 'epoch': 5.25}\n",
            "\n",
            " 26% 168/640 [03:49<09:39,  1.23s/it]\u001b[A\n",
            " 26% 169/640 [03:49<08:30,  1.08s/it]\u001b[A\n",
            " 27% 170/640 [03:50<07:42,  1.02it/s]\u001b[A\n",
            " 27% 171/640 [03:51<07:08,  1.09it/s]\u001b[A\n",
            " 27% 172/640 [03:52<06:47,  1.15it/s]\u001b[A\n",
            " 27% 173/640 [03:52<06:32,  1.19it/s]\u001b[A\n",
            " 27% 174/640 [03:53<06:21,  1.22it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:39\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.183, 'grad_norm': 4.6857733726501465, 'learning_rate': 1.8253599330848638e-05, 'epoch': 5.4375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.183, 'grad_norm': 4.6857733726501465, 'learning_rate': 1.8253599330848638e-05, 'epoch': 5.44}\n",
            "\n",
            " 27% 174/640 [03:53<06:21,  1.22it/s]\u001b[A\n",
            " 27% 175/640 [03:54<06:13,  1.24it/s]\u001b[A\n",
            " 28% 176/640 [03:55<06:05,  1.27it/s]\u001b[A\n",
            " 28% 177/640 [03:55<06:02,  1.28it/s]\u001b[A\n",
            " 28% 178/640 [03:56<05:59,  1.29it/s]\u001b[A\n",
            " 28% 179/640 [03:57<05:57,  1.29it/s]\u001b[A\n",
            " 28% 180/640 [03:58<05:58,  1.28it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:43\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.1696, 'grad_norm': 2.0793087482452393, 'learning_rate': 1.806444604267483e-05, 'epoch': 5.625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.1696, 'grad_norm': 2.0793087482452393, 'learning_rate': 1.806444604267483e-05, 'epoch': 5.62}\n",
            "\n",
            " 28% 180/640 [03:58<05:58,  1.28it/s]\u001b[A\n",
            " 28% 181/640 [03:59<05:58,  1.28it/s]\u001b[A\n",
            " 28% 182/640 [03:59<05:54,  1.29it/s]\u001b[A\n",
            " 29% 183/640 [04:00<05:52,  1.30it/s]\u001b[A\n",
            " 29% 184/640 [04:01<05:49,  1.31it/s]\u001b[A\n",
            " 29% 185/640 [04:02<05:49,  1.30it/s]\u001b[A\n",
            " 29% 186/640 [04:02<05:49,  1.30it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:48\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.1893, 'grad_norm': 1.5989601612091064, 'learning_rate': 1.786665714368617e-05, 'epoch': 5.8125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.1893, 'grad_norm': 1.5989601612091064, 'learning_rate': 1.786665714368617e-05, 'epoch': 5.81}\n",
            "\n",
            " 29% 186/640 [04:02<05:49,  1.30it/s]\u001b[A\n",
            " 29% 187/640 [04:03<05:47,  1.30it/s]\u001b[A\n",
            " 29% 188/640 [04:04<05:48,  1.30it/s]\u001b[A\n",
            " 30% 189/640 [04:05<05:46,  1.30it/s]\u001b[A\n",
            " 30% 190/640 [04:05<05:46,  1.30it/s]\u001b[A\n",
            " 30% 191/640 [04:06<05:43,  1.31it/s]\u001b[A\n",
            " 30% 192/640 [04:06<04:33,  1.64it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 18:59:52\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.1691, 'grad_norm': 4.12998628616333, 'learning_rate': 1.766044443118978e-05, 'epoch': 6.0}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.1691, 'grad_norm': 4.12998628616333, 'learning_rate': 1.766044443118978e-05, 'epoch': 6.0}\n",
            "\n",
            "100% 16/16 [00:07<00:00,  2.33it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:00\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.5137507319450378, 'eval_f1_macro': 0.6267262455012527, 'eval_f1_micro': 0.876, 'eval_f1_weighted': 0.8719996622714368, 'eval_precision_macro': 0.6938468598682456, 'eval_precision_micro': 0.876, 'eval_precision_weighted': 0.874647660631277, 'eval_recall_macro': 0.5972755639842384, 'eval_recall_micro': 0.876, 'eval_recall_weighted': 0.876, 'eval_accuracy': 0.876, 'eval_runtime': 7.98, 'eval_samples_per_second': 125.313, 'eval_steps_per_second': 2.005, 'epoch': 6.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.5137507319450378, 'eval_f1_macro': 0.6267262455012527, 'eval_f1_micro': 0.876, 'eval_f1_weighted': 0.8719996622714368, 'eval_precision_macro': 0.6938468598682456, 'eval_precision_micro': 0.876, 'eval_precision_weighted': 0.874647660631277, 'eval_recall_macro': 0.5972755639842384, 'eval_recall_micro': 0.876, 'eval_recall_weighted': 0.876, 'eval_accuracy': 0.876, 'eval_runtime': 7.98, 'eval_samples_per_second': 125.313, 'eval_steps_per_second': 2.005, 'epoch': 6.0}\n",
            "100% 16/16 [00:07<00:00,  2.33it/s]\n",
            "\n",
            " 30% 193/640 [04:38<1:13:47,  9.90s/it]\u001b[A\n",
            " 30% 194/640 [04:39<53:09,  7.15s/it]  \u001b[A\n",
            " 30% 195/640 [04:39<38:48,  5.23s/it]\u001b[A\n",
            " 31% 196/640 [04:40<28:46,  3.89s/it]\u001b[A\n",
            " 31% 197/640 [04:41<21:45,  2.95s/it]\u001b[A\n",
            " 31% 198/640 [04:42<16:51,  2.29s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:27\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.1195, 'grad_norm': 2.431419849395752, 'learning_rate': 1.7446028722923266e-05, 'epoch': 6.1875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.1195, 'grad_norm': 2.431419849395752, 'learning_rate': 1.7446028722923266e-05, 'epoch': 6.19}\n",
            "\n",
            " 31% 198/640 [04:42<16:51,  2.29s/it]\u001b[A\n",
            " 31% 199/640 [04:43<13:27,  1.83s/it]\u001b[A\n",
            " 31% 200/640 [04:43<11:03,  1.51s/it]\u001b[A\n",
            " 31% 201/640 [04:44<09:22,  1.28s/it]\u001b[A\n",
            " 32% 202/640 [04:45<08:12,  1.12s/it]\u001b[A\n",
            " 32% 203/640 [04:46<07:24,  1.02s/it]\u001b[A\n",
            " 32% 204/640 [04:46<06:51,  1.06it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.1518, 'grad_norm': 4.213328838348389, 'learning_rate': 1.7223639620597556e-05, 'epoch': 6.375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.1518, 'grad_norm': 4.213328838348389, 'learning_rate': 1.7223639620597556e-05, 'epoch': 6.38}\n",
            "\n",
            " 32% 204/640 [04:46<06:51,  1.06it/s]\u001b[A\n",
            " 32% 205/640 [04:47<06:27,  1.12it/s]\u001b[A\n",
            " 32% 206/640 [04:48<06:07,  1.18it/s]\u001b[A\n",
            " 32% 207/640 [04:49<05:56,  1.21it/s]\u001b[A\n",
            " 32% 208/640 [04:49<05:46,  1.25it/s]\u001b[A\n",
            " 33% 209/640 [04:50<05:41,  1.26it/s]\u001b[A\n",
            " 33% 210/640 [04:51<05:37,  1.27it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:36\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.107, 'grad_norm': 0.6892173886299133, 'learning_rate': 1.699351526403367e-05, 'epoch': 6.5625}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.107, 'grad_norm': 0.6892173886299133, 'learning_rate': 1.699351526403367e-05, 'epoch': 6.56}\n",
            "\n",
            " 33% 210/640 [04:51<05:37,  1.27it/s]\u001b[A\n",
            " 33% 211/640 [04:52<05:35,  1.28it/s]\u001b[A\n",
            " 33% 212/640 [04:52<05:32,  1.29it/s]\u001b[A\n",
            " 33% 213/640 [04:53<05:31,  1.29it/s]\u001b[A\n",
            " 33% 214/640 [04:54<05:29,  1.29it/s]\u001b[A\n",
            " 34% 215/640 [04:55<05:29,  1.29it/s]\u001b[A\n",
            " 34% 216/640 [04:56<05:27,  1.30it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:41\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0936, 'grad_norm': 0.9574422836303711, 'learning_rate': 1.6755902076156606e-05, 'epoch': 6.75}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0936, 'grad_norm': 0.9574422836303711, 'learning_rate': 1.6755902076156606e-05, 'epoch': 6.75}\n",
            "\n",
            " 34% 216/640 [04:56<05:27,  1.30it/s]\u001b[A\n",
            " 34% 217/640 [04:56<05:27,  1.29it/s]\u001b[A\n",
            " 34% 218/640 [04:57<05:27,  1.29it/s]\u001b[A\n",
            " 34% 219/640 [04:58<05:28,  1.28it/s]\u001b[A\n",
            " 34% 220/640 [04:59<05:28,  1.28it/s]\u001b[A\n",
            " 35% 221/640 [04:59<05:29,  1.27it/s]\u001b[A\n",
            " 35% 222/640 [05:00<05:25,  1.28it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:46\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0756, 'grad_norm': 0.8393335342407227, 'learning_rate': 1.6511054499119493e-05, 'epoch': 6.9375}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0756, 'grad_norm': 0.8393335342407227, 'learning_rate': 1.6511054499119493e-05, 'epoch': 6.94}\n",
            "\n",
            " 35% 222/640 [05:00<05:25,  1.28it/s]\u001b[A\n",
            " 35% 223/640 [05:01<05:23,  1.29it/s]\u001b[A\n",
            "100% 16/16 [00:07<00:00,  2.23it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:00:55\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.5909944176673889, 'eval_f1_macro': 0.62497864130932, 'eval_f1_micro': 0.862, 'eval_f1_weighted': 0.8577589525990966, 'eval_precision_macro': 0.7124500847649677, 'eval_precision_micro': 0.862, 'eval_precision_weighted': 0.8656079769288854, 'eval_recall_macro': 0.5842627521138469, 'eval_recall_micro': 0.862, 'eval_recall_weighted': 0.862, 'eval_accuracy': 0.862, 'eval_runtime': 8.2094, 'eval_samples_per_second': 121.812, 'eval_steps_per_second': 1.949, 'epoch': 7.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.5909944176673889, 'eval_f1_macro': 0.62497864130932, 'eval_f1_micro': 0.862, 'eval_f1_weighted': 0.8577589525990966, 'eval_precision_macro': 0.7124500847649677, 'eval_precision_micro': 0.862, 'eval_precision_weighted': 0.8656079769288854, 'eval_recall_macro': 0.5842627521138469, 'eval_recall_micro': 0.862, 'eval_recall_weighted': 0.862, 'eval_accuracy': 0.862, 'eval_runtime': 8.2094, 'eval_samples_per_second': 121.812, 'eval_steps_per_second': 1.949, 'epoch': 7.0}\n",
            "100% 16/16 [00:07<00:00,  2.23it/s]\n",
            "\n",
            " 35% 225/640 [05:36<1:14:55, 10.83s/it]\u001b[A\n",
            " 35% 226/640 [05:37<53:50,  7.80s/it]  \u001b[A\n",
            " 35% 227/640 [05:37<39:09,  5.69s/it]\u001b[A\n",
            " 36% 228/640 [05:38<28:54,  4.21s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:24\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0578, 'grad_norm': 2.1850719451904297, 'learning_rate': 1.6259234721840595e-05, 'epoch': 7.125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0578, 'grad_norm': 2.1850719451904297, 'learning_rate': 1.6259234721840595e-05, 'epoch': 7.12}\n",
            "\n",
            " 36% 228/640 [05:38<28:54,  4.21s/it]\u001b[A\n",
            " 36% 229/640 [05:39<21:42,  3.17s/it]\u001b[A\n",
            " 36% 230/640 [05:40<16:42,  2.45s/it]\u001b[A\n",
            " 36% 231/640 [05:40<13:12,  1.94s/it]\u001b[A\n",
            " 36% 232/640 [05:41<10:44,  1.58s/it]\u001b[A\n",
            " 36% 233/640 [05:42<09:02,  1.33s/it]\u001b[A\n",
            " 37% 234/640 [05:43<07:50,  1.16s/it]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:28\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0473, 'grad_norm': 0.5393295884132385, 'learning_rate': 1.6000712399244813e-05, 'epoch': 7.3125}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0473, 'grad_norm': 0.5393295884132385, 'learning_rate': 1.6000712399244813e-05, 'epoch': 7.31}\n",
            "\n",
            " 37% 234/640 [05:43<07:50,  1.16s/it]\u001b[A\n",
            " 37% 235/640 [05:43<06:59,  1.04s/it]\u001b[A\n",
            " 37% 236/640 [05:44<06:26,  1.05it/s]\u001b[A\n",
            " 37% 237/640 [05:45<06:02,  1.11it/s]\u001b[A\n",
            " 37% 238/640 [05:46<05:46,  1.16it/s]\u001b[A\n",
            " 37% 239/640 [05:46<05:34,  1.20it/s]\u001b[A\n",
            " 38% 240/640 [05:47<05:25,  1.23it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:33\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0717, 'grad_norm': 1.1762418746948242, 'learning_rate': 1.573576436351046e-05, 'epoch': 7.5}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0717, 'grad_norm': 1.1762418746948242, 'learning_rate': 1.573576436351046e-05, 'epoch': 7.5}\n",
            "\n",
            " 38% 240/640 [05:47<05:25,  1.23it/s]\u001b[A\n",
            " 38% 241/640 [05:48<05:19,  1.25it/s]\u001b[A\n",
            " 38% 242/640 [05:49<05:15,  1.26it/s]\u001b[A\n",
            " 38% 243/640 [05:50<05:11,  1.27it/s]\u001b[A\n",
            " 38% 244/640 [05:50<05:09,  1.28it/s]\u001b[A\n",
            " 38% 245/640 [05:51<05:06,  1.29it/s]\u001b[A\n",
            " 38% 246/640 [05:52<05:05,  1.29it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:37\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0719, 'grad_norm': 19.51105308532715, 'learning_rate': 1.5464674327630437e-05, 'epoch': 7.6875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0719, 'grad_norm': 19.51105308532715, 'learning_rate': 1.5464674327630437e-05, 'epoch': 7.69}\n",
            "\n",
            " 38% 246/640 [05:52<05:05,  1.29it/s]\u001b[A\n",
            " 39% 247/640 [05:53<05:05,  1.29it/s]\u001b[A\n",
            " 39% 248/640 [05:53<05:04,  1.29it/s]\u001b[A\n",
            " 39% 249/640 [05:54<05:03,  1.29it/s]\u001b[A\n",
            " 39% 250/640 [05:55<05:01,  1.29it/s]\u001b[A\n",
            " 39% 251/640 [05:56<05:03,  1.28it/s]\u001b[A\n",
            " 39% 252/640 [05:57<05:01,  1.29it/s]\u001b[A\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:42\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'loss': 0.0636, 'grad_norm': 3.7712948322296143, 'learning_rate': 1.5187732581605217e-05, 'epoch': 7.875}\u001b[0m\n",
            "\n",
            "\u001b[A{'loss': 0.0636, 'grad_norm': 3.7712948322296143, 'learning_rate': 1.5187732581605217e-05, 'epoch': 7.88}\n",
            "\n",
            " 39% 252/640 [05:57<05:01,  1.29it/s]\u001b[A\n",
            " 40% 253/640 [05:57<05:03,  1.27it/s]\u001b[A\n",
            " 40% 254/640 [05:58<05:06,  1.26it/s]\u001b[A\n",
            " 40% 255/640 [05:59<05:04,  1.26it/s]\u001b[A\n",
            "100% 16/16 [00:07<00:00,  2.23it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:01:53\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.6073558926582336, 'eval_f1_macro': 0.6370735106832883, 'eval_f1_micro': 0.859, 'eval_f1_weighted': 0.8621121313092627, 'eval_precision_macro': 0.6905472611770643, 'eval_precision_micro': 0.859, 'eval_precision_weighted': 0.8739038428069906, 'eval_recall_macro': 0.6215983024984887, 'eval_recall_micro': 0.859, 'eval_recall_weighted': 0.859, 'eval_accuracy': 0.859, 'eval_runtime': 8.2036, 'eval_samples_per_second': 121.898, 'eval_steps_per_second': 1.95, 'epoch': 8.0}\u001b[0m\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.6073558926582336, 'eval_f1_macro': 0.6370735106832883, 'eval_f1_micro': 0.859, 'eval_f1_weighted': 0.8621121313092627, 'eval_precision_macro': 0.6905472611770643, 'eval_precision_micro': 0.859, 'eval_precision_weighted': 0.8739038428069906, 'eval_recall_macro': 0.6215983024984887, 'eval_recall_micro': 0.859, 'eval_recall_weighted': 0.859, 'eval_accuracy': 0.859, 'eval_runtime': 8.2036, 'eval_samples_per_second': 121.898, 'eval_steps_per_second': 1.95, 'epoch': 8.0}\n",
            "100% 16/16 [00:07<00:00,  2.23it/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:02:18\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'train_runtime': 392.8839, 'train_samples_per_second': 50.906, 'train_steps_per_second': 1.629, 'train_loss': 0.6052985875867307, 'epoch': 8.0}\u001b[0m\n",
            "\n",
            "\u001b[A{'train_runtime': 392.8839, 'train_samples_per_second': 50.906, 'train_steps_per_second': 1.629, 'train_loss': 0.6052985875867307, 'epoch': 8.0}\n",
            "\n",
            " 40% 256/640 [06:32<09:49,  1.53s/it]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:02:18\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1mFinished training, saving model...\u001b[0m\n",
            "100% 16/16 [00:07<00:00,  2.35it/s]/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:02:32\u001b[0m | \u001b[36mautotrain.trainers.common\u001b[0m:\u001b[36mon_log\u001b[0m:\u001b[36m367\u001b[0m - \u001b[1m{'eval_loss': 0.5137507319450378, 'eval_f1_macro': 0.6267262455012527, 'eval_f1_micro': 0.876, 'eval_f1_weighted': 0.8719996622714368, 'eval_precision_macro': 0.6938468598682456, 'eval_precision_micro': 0.876, 'eval_precision_weighted': 0.874647660631277, 'eval_recall_macro': 0.5972755639842384, 'eval_recall_micro': 0.876, 'eval_recall_weighted': 0.876, 'eval_accuracy': 0.876, 'eval_runtime': 7.6842, 'eval_samples_per_second': 130.137, 'eval_steps_per_second': 2.082, 'epoch': 8.0}\u001b[0m\n",
            "100% 16/16 [00:07<00:00,  2.25it/s]\n",
            "\u001b[1mINFO    \u001b[0m | \u001b[32m2026-01-06 19:02:34\u001b[0m | \u001b[36mautotrain.parser\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m229\u001b[0m - \u001b[1mJob ID: 3827\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!autotrain --config config.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r news-labeler-high-acc.zip ./news-labeler-high-acc/"
      ],
      "metadata": {
        "id": "TJhDFYjbuEVx",
        "outputId": "12e9cd60-8182-4141-ac96-d1074a92197d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: news-labeler-high-acc/ (stored 0%)\n",
            "  adding: news-labeler-high-acc/tokenizer_config.json (deflated 74%)\n",
            "  adding: news-labeler-high-acc/vocab.txt (deflated 62%)\n",
            "  adding: news-labeler-high-acc/runs/ (stored 0%)\n",
            "  adding: news-labeler-high-acc/runs/Jan06_18-55-44_1f5f91d9c1fc/ (stored 0%)\n",
            "  adding: news-labeler-high-acc/runs/Jan06_18-55-44_1f5f91d9c1fc/events.out.tfevents.1767726152.1f5f91d9c1fc.3864.1 (deflated 45%)\n",
            "  adding: news-labeler-high-acc/runs/Jan06_18-55-44_1f5f91d9c1fc/events.out.tfevents.1767725745.1f5f91d9c1fc.3864.0 (deflated 66%)\n",
            "  adding: news-labeler-high-acc/training_params.json (deflated 52%)\n",
            "  adding: news-labeler-high-acc/tokenizer.json (deflated 72%)\n",
            "  adding: news-labeler-high-acc/config.json (deflated 54%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/ (stored 0%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/rng_state.pth (deflated 26%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/optimizer.pt (deflated 48%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/config.json (deflated 54%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/trainer_state.json (deflated 76%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/scheduler.pt (deflated 61%)\n",
            "  adding: news-labeler-high-acc/checkpoint-224/model.safetensors"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}